Please make sure you have all relevant .txt files provided in the project in the same directory.

If you wish to have the same output as the screenshots in the report, be sure to change print statements.

All generated Excel files are in the 'Excel' folder; some parts will require moving these files
into the directory of the code; this is detailed if necessary for each part.

Part 1: part1.py

Description: For each hashtag you want to analyze, change the arguments of 'AverageTweetsPerHour' in 
the main function. For example, if you want to analyze #superbowl, change the file name and the text
that will be displayed; the output of the code will show the statistics for that hashtag.

For the two hashtags that you want to generate the histogram for, uncomment the histogram section in the
'AverageTweetsPerHour' function and change the name of the Excel file accordingly. Further formatting in
Excel is necessary in order to generate the histogram; the histogram Excel files have been included.

Part 2: part2.py

Description: For each hashtag you want to analyze, change the 'hashtags' variable, the arguments of the
'ConstructData' function. You will need to provide the timespan of each hashtag; they are tabled below:

gohawks = 973
gopatriots = 684
nfl = 927
patriots = 981
sb49 = 583
superbowl - 963
all hashtags = 981

Each instance of the code will generate the feature vectors and regression values and place them in an
Excel file named with the hashtag. For example, #gohawks will generate gohawks.xlsx. Then, the RMSE value
of the model generated using the dataset from that hashtag will be displayed.

All of these Excel files are included.

Part 3: part3.py

Description: Same procedure as part2.py, only this time including more features. The only difference is the
naming convention of the Excel files generated; they now include a 'New' suffix (i.e. gohawksNew.xlsx)

Part 4: part4a.py and part4b.py

Description: part4a.py will do the cross validation for all periods together. Change 'filename' for the hashtag
that you wish to analyze. Be sure to have the Excel file that you want to analyze in the same directory. Also,
use the 'New' suffix Excel files as detailed in the report.

This will generate the average prediction error for each test, then the average over all tests.

part4b.py will do the CV for each period separately. Again, you will need to have the Excel file in the same
directory and change the names appropriately. Thus, 3 different sets of 10 tests will be generated for the given
hashtag.

Part 5: part5.py and part5s.py

Description: Ensure that the filename is 'allhashtagsNew6.xlsx' due to the testing data containing 6 hour windows.
Change the 'testname' and 'lin.fit' parameters for each sample file. For example, for sample1_period1, change the 
testname to 'sample1_period1.txt' and the lin.fit parameters to obs1 and target1 indicating that we are using the
model for the 1st period. This is what part5.py does.

For part5s.py, this code simply prints the statistics for each sample tweet set. This was done in order to make sense
of our predictions.

Also, ensure that the Excel file is present as well (allhashtagsNew6.xlsx) in the same directory.

Part 6: part6.py

Description: This code first extracts the tweets that meet our specficiations (location being in MA or WA) and pickles them 
(tweetText & tru_loc). These two files have been pregenerated for the grader's convenience.



The function call: fanBasePredictorPack('tweets_#superbowl.txt') therefore only needs to be run once as indicated in the code.


The rest of the code creates, trains, and tests our soft-margin SVM classifier which outputs the excel files: 
'superbowlLocation.xlsx' and 'ROC Curve LSVM.xlsx'. These files have also been pregenerated for your convenience. 

Part 7: part7a.py and part7b.py

Description: part7a.py generates the feature vectors and corresponding regression value, trains a model, then reports the 
RMSE value for attempting to predict emotional levels. You can change the 'hashtags' variable if you wish to analyze a certain
hashtag alone.

part7b.py displays the CV average predicted error and the averaged error over all tests. Make sure the Excel file generated by
part7a.py is present in the directory.

Thank you for a great quarter!

 